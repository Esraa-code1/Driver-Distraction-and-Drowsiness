# -*- coding: utf-8 -*-
"""CNN_on_Distracted_Driver_Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/cnn-on-distracted-driver-dataset-894f607d-506d-4c47-96f8-863e11708e39.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240625/auto/storage/goog4_request%26X-Goog-Date%3D20240625T145252Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0aed2c2b8dcb8a2a576e7ed8c3b0c7dd4507c6bbaf941b0cd9759b6f4dc23922e6a09a71cdbf52201be25e4870444c5ff9de115316c1eadcb3cc43858fdebd1861cda67f654892e1ec9e06245b682558989e73751b4578bf74a85d70dbd7887d8daaf8b64f930f32781f2bbdabcf273eb36cab3de8a0244bad44bb3eb7bc281eab3305856ff3cfa4ba43b1bf73eddcc9b8f6686570d3d9261b2a9b12c80e9d556b6e85ece1d276ef8f876e59b604e89a99d26081695b6f6b59ef365d44652e62d0e9fd5fbf7326342d03fa5fd4b50896a0f638429e12ac0165453f4dd612a58b40436f457059f92fd1e75cc5b26b1c5357ae60280d7377e3150bed1748832b57
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'state-farm-distracted-driver-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F5048%2F868335%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240625%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240625T145252Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8245c4fb95f83e1ee4a85f5340109f978b83fe470553ecccece3d0ad5ab9397b8b832f0fd94e1d92be75f6e6846a3ef7a6ffddc6760c521adfd153e23032e7586b9168636644cb6c1d0c0fcdf538fb2306b3b9556c01fa2afbb36dccac4aefb2f9a3b4e9b0271705d895f2ff09342208f05cab65cc623cc90ad3e57152a4c9df74b833c96176a7725e96c557c960a8b0d36cba78f528b431324fd8253afe34fd49e70a05ba4f9b65022060c64bcb906580311d1769bbdb13b67dd257f02d4462a03ed396232d8b8e83b33aac0e70af0e1cbed191eef69bbc4add0bf9ac9442a657ee744ff086afe65e669450e83d423471fb154b350ba56972bfa4cc917f8271'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session



"""## Import Necessary Libraries"""

import os
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping

"""## Explore the Dataset"""

next(os.walk('/kaggle/input/state-farm-distracted-driver-detection'))

path='/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv'
df=pd.read_csv(path)
df.head()

len(df.subject.value_counts().keys())

"""### Hence, number of unique drivers is 26

## Display the Class Paths
"""

next(os.walk('/kaggle/input/state-farm-distracted-driver-detection/imgs'))

next(os.walk('/kaggle/input/state-farm-distracted-driver-detection/imgs/train'))

class_paths=[]
for dirname, _, filenames in os.walk('/kaggle/input/state-farm-distracted-driver-detection/imgs/train'):
    for a in _:
        cls_path=os.path.join(dirname, a)
        print(cls_path)
        class_paths.append(cls_path)

a,b,c=next(os.walk(class_paths[0]))
a,b,c[:10]

"""## Display Sample Image"""

image1=cv2.cvtColor(cv2.imread('/kaggle/input/state-farm-distracted-driver-detection/imgs/train/c5/img_68208.jpg'), cv2.COLOR_BGR2RGB)
plt.imshow(image1)
plt.show()
image1.shape

"""## Prepare Training Data"""

train1=[]
labels=[]
for class_path in class_paths:
    a_,b_,c_=next(os.walk(class_path))
#     print(a_,b_,c_)
    for c__ in c_:
        img_path=os.path.join(a_, c__)
#         print(a_[-1:], img_path)
        img=cv2.imread(img_path, cv2.IMREAD_COLOR)
        img=cv2.resize(img, (96,96))
        train1.append(img)
        labels.append(int(a_[-1:]))
#         break

plt.imshow(cv2.cvtColor(train1[0], cv2.COLOR_BGR2RGB))
plt.show()

print(np.array(labels).shape)
print(labels[:10])

"""## One-Hot Encoding"""

y = tf.keras.utils.to_categorical(labels, 10)
print(type(y))
y[:10]

"""## Normalize Data"""

# train=np.array(train, dtype=np.uint8).reshape(-1, 160, 160, 3)
train=np.array(train1, dtype=np.float32).reshape(-1, 96, 96, 3)
print(np.max(train))
train=train/255
print(np.max(train))

plt.imshow(cv2.cvtColor(train[0], cv2.COLOR_BGR2RGB))
plt.show()

type(train)

set(labels)

"""## Split Data into Train and Test Sets"""

X_train, X_test, y_train, y_test=train_test_split(train, y, test_size=0.2, random_state=0, stratify=labels)
print("Train label distribution:", np.sum(y_train, axis=0))
print("Test label distribution:", np.sum(y_test, axis=0))

"""## Build CNN Model"""

model=Sequential()

model.add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))
model.add(Conv232, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.3))

model.add(Conv2D(64, kernel_size=3, padding='same', activation='relu'))
model.add(Conv2D(54, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.3))

model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))
model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())

model.add(Dense(512,activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(10, activation='softmax'))

model.summary()

"""## Compile the Model"""

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

"""## Set Up Callbacks"""

checkpointer=ModelCheckpoint(
    'saved_models/weights_best_vanilla.keras',
#     monitor='val_loss',
    monitor='val_loss',
    verbose=1,
    save_best_only=True,
    mode='auto',
)

es=EarlyStopping(monitor='val_loss',mode='auto',verbose=1,patience=2,restore_best_weights=True)

X_train.shape

"""## Train the Model"""

# model.fit(
#     x=X_train,
#     y=y_train,
#     batch_size=40,
#     epochs=10,
#     verbose=1,
#     callbacks=[checkpointer, es],
# #     callbacks=[ checkpointer],
#     validation_data=(X_test, y_test),
# #     shuffle=True,
# )
model.fit(
    x=X_train,
    y=y_train,
    batch_size=40,
    epochs=20,
    verbose='auto',
    callbacks=[checkpointer, es],
    validation_split=0.1,
#     validation_data=None,
    shuffle=True,
#     class_weight=None,
#     sample_weight=None,
#     initial_epoch=0,
#     steps_per_epoch=None,
#     validation_steps=None,
#     validation_batch_size=None,
#     validation_freq=1
)

"""## Evaluate the Model"""



model.evaluate(X_test, y_test)



#################################################################################################################

# -*- coding: utf-8 -*-
"""Copy of driver drowsiness using keras

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sykULklxZZWYi7fMNzIgLyZtsfyoAaXA
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'drowsiness-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F896350%2F1520586%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240625%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240625T145723Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D10aef2a5ab5f4f5a0987156b124bb4845ffd0290f3077c71f172ea4e88c24ed2dbc8711c23fb8e53bb0aa0ad94653c914059334264f9096dac4265bb7006e96633074aa04d3168ccab433e0ddb151014688f29b9fd141a0e6bbac44d573677a3fa61e6cdd22876dc54c682d92811a442e27b93000dcd33ec36a9d1542eca2801a0d9045876ade1bc19627472f4bea94d39905d57f8c6ee460bcbc389897dba56d6058adfae5b7a5bcf3f142cfc98a37f761177d9aee1c1a2d5f8e32652ff19fc798a23840c698b6c2058a0b8e5cbd9f496b7fe60e8e58a4e9532ddc5a2ebe0105c8419594f00268f81edd81ac031717e459e12ec9d159077cbf81d138dceb69e,prediction-images:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F839168%2F2935731%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240625%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240625T145723Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7b5db379d63284cb59c9e087fffe2675f574d99f4514c7e582fb07ed8dc4e0306a808dbbee24254006d308d24aa68b773b3b7866e1f65360fa095a8228b6494acd8b6b880196517dea4261adf6b9a7ca067d48f51dc9375503f6074de0564f82cd6307f46466f6811ebd2614d21265953a25a263b3aa842cbdb1555b37b53888e451e67b15ff00ae76bd620af0f3ea04fe3b3ea0d84945b3b8b199f3e8fb2d3ae3bb9e4faba7d56c28d8cba77d09fd67d89426f857c8d1622fddcfc464fdd976a093f495273dcc5d6ac3182d066af0b185f1402d44cdacd3649468b96fef0d68d0950a18b258365306b2e0e552cf9ec051043180517365a19dc630e30690655f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""![fatigued-truck-driver-1.gif](attachment:fatigued-truck-driver-1.gif)

## Driver drowsiness detection is a car safety technology which helps prevent accidents caused by the driver getting drowsy. Various studies have suggested that around 20% of all road accidents are fatigue-related, up to 50% on certain roads.
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import cv2

"""# labels"""

labels = os.listdir("../input/drowsiness-dataset/train")

labels

"""# visualize random 1 image"""

import matplotlib.pyplot as plt
plt.imshow(plt.imread("../input/drowsiness-dataset/train/Closed/_0.jpg"))

"""# image array"""

a = plt.imread("../input/drowsiness-dataset/train/yawn/10.jpg")

"""# image shape"""

a.shape

"""# visualize yawn image.
# Here background is unnecessary. we need only face image array
"""

plt.imshow(plt.imread("../input/drowsiness-dataset/train/yawn/10.jpg"))

"""# for yawn and not_yawn. Take only face"""

def face_for_yawn(direc="../input/drowsiness-dataset/train", face_cas_path="../input/prediction-images/haarcascade_frontalface_default.xml"):
    yaw_no = []
    IMG_SIZE = 145
    categories = ["yawn", "no_yawn"]
    for category in categories:
        path_link = os.path.join(direc, category)
        class_num1 = categories.index(category)
        print(class_num1)
        for image in os.listdir(path_link):
            image_array = cv2.imread(os.path.join(path_link, image), cv2.IMREAD_COLOR)
            face_cascade = cv2.CascadeClassifier(face_cas_path)
            faces = face_cascade.detectMultiScale(image_array, 1.3, 5)
            for (x, y, w, h) in faces:
                img = cv2.rectangle(image_array, (x, y), (x+w, y+h), (0, 255, 0), 2)
                roi_color = img[y:y+h, x:x+w]
                resized_array = cv2.resize(roi_color, (IMG_SIZE, IMG_SIZE))
                yaw_no.append([resized_array, class_num1])
    return yaw_no


yawn_no_yawn = face_for_yawn()

"""# for closed and open eye"""

def get_data(dir_path="../input/drowsiness-dataset/train/", face_cas="../input/prediction-images/haarcascade_frontalface_default.xml", eye_cas="../input/prediction-images/haarcascade.xml"):
    labels = ['Closed', 'Open']
    IMG_SIZE = 145
    data = []
    for label in labels:
        path = os.path.join(dir_path, label)
        class_num = labels.index(label)
        class_num +=2
        print(class_num)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)
                resized_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                data.append([resized_array, class_num])
            except Exception as e:
                print(e)
    return data

data_train = get_data()

"""# extend data and convert array"""

def append_data():
#     total_data = []
    yaw_no = face_for_yawn()
    data = get_data()
    yaw_no.extend(data)
    return np.array(yaw_no)

"""# new variable to store"""

new_data = append_data()

"""# separate label and features"""

X = []
y = []
for feature, label in new_data:
    X.append(feature)
    y.append(label)

"""# reshape the array"""

X = np.array(X)
X = X.reshape(-1, 145, 145, 3)

"""# LabelBinarizer"""

from sklearn.preprocessing import LabelBinarizer
label_bin = LabelBinarizer()
y = label_bin.fit_transform(y)

"""# label array"""

y = np.array(y)

"""# train test split"""

from sklearn.model_selection import train_test_split
seed = 42
test_size = 0.30
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size)

"""# length of X_test"""

len(X_test)

"""# Not necessary, only use to matching with my pc version"""

# !pip install tensorflow==2.3.1
# !pip install keras==2.4.3

"""# import some dependencies"""

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

"""# tensorflow version"""

tf.__version__

"""# keras version"""

import keras
keras.__version__

"""# Data Augmentation"""

train_generator = ImageDataGenerator(rescale=1/255, zoom_range=0.2, horizontal_flip=True, rotation_range=30)
test_generator = ImageDataGenerator(rescale=1/255)

train_generator = train_generator.flow(np.array(X_train), y_train, shuffle=False)
test_generator = test_generator.flow(np.array(X_test), y_test, shuffle=False)

"""# Model"""

model = Sequential()

model.add(Conv2D(256, (3, 3), activation="relu", input_shape=X_train.shape[1:]))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(128, (3, 3), activation="relu"))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(64, (3, 3), activation="relu"))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(32, (3, 3), activation="relu"))
model.add(MaxPooling2D(2, 2))

model.add(Flatten())
model.add(Dropout(0.5))

model.add(Dense(64, activation="relu"))
model.add(Dense(4, activation="softmax"))

model.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer="adam")

model.summary()

history = model.fit(train_generator, epochs=50, validation_data=test_generator, shuffle=True, validation_steps=len(test_generator))

"""# history"""

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(accuracy))

plt.plot(epochs, accuracy, "b", label="trainning accuracy")
plt.plot(epochs, val_accuracy, "r", label="validation accuracy")
plt.legend()
plt.show()

plt.plot(epochs, loss, "b", label="trainning loss")
plt.plot(epochs, val_loss, "r", label="validation loss")
plt.legend()
plt.show()

"""# save model"""

model.save("drowiness_new6.h5")

model.save("drowiness_new6.model")

"""# Prediction"""

prediction = model.predict_classes(X_test)

prediction

"""# classification report"""

labels_new = ["yawn", "no_yawn", "Closed", "Open"]

from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test, axis=1), prediction, target_names=labels_new))

"""# predicting function"""

labels_new = ["yawn", "no_yawn", "Closed", "Open"]
IMG_SIZE = 145
def prepare(filepath, face_cas="../input/prediction-images/haarcascade_frontalface_default.xml"):
    img_array = cv2.imread(filepath, cv2.IMREAD_COLOR)
    img_array = img_array / 255
    resized_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
    return resized_array.reshape(-1, IMG_SIZE, IMG_SIZE, 3)

model = tf.keras.models.load_model("./drowiness_new6.h5")

"""# Prediction
## 0-yawn, 1-no_yawn, 2-Closed, 3-Open
"""

# prepare("../input/drowsiness-dataset/train/no_yawn/1068.jpg")
prediction = model.predict([prepare("../input/drowsiness-dataset/train/no_yawn/1067.jpg")])
np.argmax(prediction)

prediction = model.predict([prepare("../input/drowsiness-dataset/train/Closed/_101.jpg")])
np.argmax(prediction)

prediction = model.predict([prepare("../input/drowsiness-dataset/train/Open/_104.jpg")])
np.argmax(prediction)

prediction = model.predict([prepare("../input/drowsiness-dataset/train/yawn/113.jpg")])
np.argmax(prediction)

"""# If you like please upvote"""
